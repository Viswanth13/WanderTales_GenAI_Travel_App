{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cw8jbDFPWTqF",
        "outputId": "a433d570-90cc-43b9-e35a-fd79c39b5e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting serpapi\n",
            "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting amadeus\n",
            "  Downloading amadeus-12.0.0.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia-api, amadeus\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=f86068c39e9665db55fb233e956bafe11664bca7ed189c6e264e9f41ab01f645\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for amadeus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amadeus: filename=amadeus-12.0.0-py2.py3-none-any.whl size=67478 sha256=1bf9fd4a86a894dde234eea6af3402523218c6ed1830da219191e8802022ba0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/3d/ca/0c01ed3dde8eadb37517c80c1bab31308fc99f0ee31e539850\n",
            "Successfully built wikipedia-api amadeus\n",
            "Installing collected packages: watchdog, onnx, humanfriendly, faiss-cpu, amadeus, wikipedia-api, serpapi, pydeck, gtts, coloredlogs, openai, onnxruntime, streamlit\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.75.0\n",
            "    Uninstalling openai-1.75.0:\n",
            "      Successfully uninstalled openai-1.75.0\n",
            "Successfully installed amadeus-12.0.0 coloredlogs-15.0.1 faiss-cpu-1.10.0 gtts-2.5.4 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.1 openai-1.76.0 pydeck-0.9.1 serpapi-0.1.5 streamlit-1.44.1 watchdog-6.0.0 wikipedia-api-0.8.1\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.55)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.76.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.14 tiktoken-0.9.0\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.1.31)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=924498f5ac53ad7130267c01e00ec722e1a1cb3b572bbfa6965240d9bbd9d4f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai wikipedia-api serpapi requests gtts onnx onnxruntime huggingface_hub streamlit amadeus faiss-cpu\n",
        "!pip install --upgrade langchain-openai\n",
        "!pip install --upgrade google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile req.txt\n",
        "streamlit\n",
        "openai\n",
        "wikipedia-api\n",
        "faiss-cpu\n",
        "transformers\n",
        "sentence-transformers\n",
        "onnx\n",
        "onnxruntime\n",
        "numpy\n",
        "pillow\n",
        "moviepy\n",
        "gtts\n",
        "requests\n",
        "langchain\n",
        "langchain-openai\n",
        "amadeus\n",
        "graphviz\n",
        "seaborn\n",
        "scikit-learn\n",
        "matplotlib\n",
        "plotly\n",
        "gradio\n",
        "diffusers\n",
        "accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csxZsl9z3LYC",
        "outputId": "c09b70e7-7e1e-4ecf-a640-85662a17480d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing req.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j0bNq0h03cxx",
        "outputId": "f586243e-225a-469c-8b33-cec33b684006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.33)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.22 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from moviepy.editor import AudioClip"
      ],
      "metadata": {
        "id": "ODSeFTiaUbHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch\n",
        "print(\"✅ GoogleSearch is imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuC9hcgqZAYL",
        "outputId": "a5a5f960-b114-409f-e634-bab676d3854b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GoogleSearch is imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import wikipediaapi\n",
        "import serpapi\n",
        "import requests\n",
        "import gtts\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import huggingface_hub\n",
        "import streamlit\n",
        "import amadeus\n",
        "import faiss\n",
        "\n",
        "print(\"✅ All libraries are installed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKYQkWp3WWtc",
        "outputId": "1c2ccd6c-baaf-4b12-a0b6-5f5745395fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries are installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary files\n",
        "!touch app.py travel_story.py config.py utils.py\n"
      ],
      "metadata": {
        "id": "KJakQKMoW07T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from config import OPENAI_API_KEY\n",
        "\n",
        "# print(OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "LXJlvIf-9qo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write utils.py (Helper Functions for API Calls)**"
      ],
      "metadata": {
        "id": "IAIbWp1GYn_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import requests\n",
        "from config import OPENAI_API_KEY, GOOGLE_MAPS_API_KEY, WEATHER_API_KEY, AMADEUS_API_KEY, AMADEUS_API_SECRET, llm\n",
        "from amadeus import Client, ResponseError\n",
        "import os\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.in_memory import InMemoryDocstore  # ✅ Required for FAISS Storage\n",
        "from langchain.schema import Document\n",
        "\n",
        "amadeus = Client(client_id=AMADEUS_API_KEY, client_secret=AMADEUS_API_SECRET)\n",
        "\n",
        "# ✅ Initialize OpenAI Embedding Model\n",
        "embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# ✅ FAISS Setup\n",
        "dimension = 1536  # Ensure embedding dimensions match OpenAI embeddings\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "docstore = InMemoryDocstore({})\n",
        "index_to_docstore_id = {}\n",
        "\n",
        "vector_store = FAISS(embedding_model, faiss_index, docstore, index_to_docstore_id)\n",
        "\n",
        "# ✅ Fetch Travel Data\n",
        "def fetch_travel_data(destination):\n",
        "    \"\"\"Retrieve real-time travel-related information from APIs.\"\"\"\n",
        "\n",
        "    print(f\"Fetching travel data for {destination}...\")\n",
        "\n",
        "    # ✅ Wikipedia Data\n",
        "    wiki_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{destination.replace(' ', '_')}\"\n",
        "    wiki_response = requests.get(wiki_url)\n",
        "    wiki_data = wiki_response.json().get(\"extract\", \"No data available.\") if wiki_response.status_code == 200 else \"No data available.\"\n",
        "\n",
        "    # ✅ Google Places Data\n",
        "    google_places_url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "    params = {\"query\": f\"top attractions in {destination}\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "    google_response = requests.get(google_places_url, params=params)\n",
        "\n",
        "    places_info = \"No places found.\"\n",
        "    if google_response.status_code == 200:\n",
        "        places = google_response.json().get(\"results\", [])\n",
        "        places_info = \"\\n\".join([f\"{p['name']} - {p.get('formatted_address', 'No address')}\" for p in places[:5]])\n",
        "\n",
        "    return f\"{wiki_data}\\n\\nTop Attractions:\\n{places_info}\"\n",
        "\n",
        "# ✅ Update FAISS Index\n",
        "def update_faiss_index(destination):\n",
        "    \"\"\"Dynamically update FAISS index and store documents properly.\"\"\"\n",
        "\n",
        "    print(f\"🔄 Updating FAISS index for {destination}...\")\n",
        "    travel_data = fetch_travel_data(destination)\n",
        "\n",
        "    # ✅ Generate embeddings dynamically\n",
        "    doc_embedding = embedding_model.embed_documents([travel_data])\n",
        "    embedding_dim = len(doc_embedding[0])  # ✅ Detect embedding dimension dynamically\n",
        "\n",
        "    global faiss_index, vector_store, docstore, index_to_docstore_id\n",
        "\n",
        "    if faiss_index.is_trained and faiss_index.ntotal > 0:\n",
        "        existing_dim = faiss_index.d\n",
        "        if existing_dim != embedding_dim:\n",
        "            print(f\"⚠️ FAISS dimension mismatch! Reinitializing FAISS to {embedding_dim} dimensions.\")\n",
        "            faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "            docstore = InMemoryDocstore({})\n",
        "            index_to_docstore_id = {}\n",
        "\n",
        "    else:\n",
        "        print(f\"✅ Initializing FAISS with {embedding_dim} dimensions.\")\n",
        "        faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "        docstore = InMemoryDocstore({})\n",
        "        index_to_docstore_id = {}\n",
        "\n",
        "    # ✅ Add new embeddings to FAISS\n",
        "    faiss_index.add(np.array(doc_embedding))\n",
        "\n",
        "    # ✅ Store document properly in docstore\n",
        "    doc_id = str(faiss_index.ntotal - 1)\n",
        "    index_to_docstore_id[faiss_index.ntotal - 1] = doc_id\n",
        "    docstore._dict[doc_id] = Document(page_content=travel_data, metadata={\"doc_id\": doc_id})\n",
        "\n",
        "    # ✅ Save FAISS Index\n",
        "    vector_store = FAISS(embedding_model, faiss_index, docstore, index_to_docstore_id)\n",
        "    vector_store.save_local(\"faiss_travel_index\")\n",
        "\n",
        "    print(f\"✅ FAISS Index successfully updated with travel data for {destination}\")\n",
        "\n",
        "# ✅ Retrieve Data using FAISS\n",
        "def retrieve_relevant_docs(query):\n",
        "    \"\"\"Retrieve relevant travel data from FAISS dynamically.\"\"\"\n",
        "\n",
        "    print(f\"🔍 Retrieving relevant documents for query: {query}\")\n",
        "\n",
        "    # ✅ Load FAISS index safely\n",
        "    vector_store = FAISS.load_local(\n",
        "        \"faiss_travel_index\",\n",
        "        OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "\n",
        "    retrieved_docs = vector_store.similarity_search_with_score(query, k=2)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        print(\"❌ No relevant documents found.\")\n",
        "        return \"No relevant documents found for your query.\"\n",
        "\n",
        "    print(f\"✅ Relevant Documents Found: {retrieved_docs}\")\n",
        "    return \" \".join([doc.page_content for doc, _ in retrieved_docs])\n",
        "\n",
        "# ✅ Generate AI Travel Plan using RAG\n",
        "def generate_travel_plan_rag(origin, destination, start_date, end_date, purpose):\n",
        "    \"\"\"Use RAG to generate a detailed travel plan dynamically.\"\"\"\n",
        "\n",
        "    print(\"🔄 Generating AI Travel Plan using RAG...\")\n",
        "\n",
        "    # ✅ Retrieve Travel Information from FAISS\n",
        "    context_info = retrieve_relevant_docs(f\"Best travel itinerary for {destination}\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Create a detailed travel itinerary for {destination} from {origin} ({start_date} - {end_date}).\n",
        "    Purpose: {purpose}\n",
        "\n",
        "    Additional Travel Information:\n",
        "    {context_info}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content if hasattr(response, 'content') else str(response)\n",
        "\n",
        "# ✅ Generate AI Travel Story using RAG\n",
        "def generate_travel_story_rag(origin, destination, start_date, end_date, purpose):\n",
        "    \"\"\"Use RAG to generate a travel story dynamically.\"\"\"\n",
        "\n",
        "    print(\"🔄 Generating AI Travel Story using RAG...\")\n",
        "\n",
        "    # ✅ Retrieve Travel Information from FAISS\n",
        "    context_info = retrieve_relevant_docs(f\"Best places to visit in {destination} for {purpose}\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Create a compelling travel story about visiting {destination} from {origin} ({start_date} - {end_date}).\n",
        "    Purpose: {purpose}\n",
        "\n",
        "    Additional Travel Information:\n",
        "    {context_info}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content if hasattr(response, 'content') else str(response)\n",
        "\n",
        "\n",
        "# ✅ Function to fetch latitude & longitude\n",
        "def get_lat_lng(location):\n",
        "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "    params = {\"address\": location, \"key\": GOOGLE_MAPS_API_KEY}\n",
        "    response = requests.get(url, params=params).json()\n",
        "    if \"results\" in response and response[\"results\"]:\n",
        "        location_data = response[\"results\"][0][\"geometry\"][\"location\"]\n",
        "        return location_data[\"lat\"], location_data[\"lng\"]\n",
        "    return None, None\n",
        "\n",
        "# ✅ Function to fetch tourist attractions\n",
        "def fetch_tourist_attractions(location, top_n=5):\n",
        "    lat, lng = get_lat_lng(location)\n",
        "    if not lat or not lng:\n",
        "        return \"Could not determine the exact location.\"\n",
        "\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\"location\": f\"{lat},{lng}\", \"radius\": 10000, \"type\": \"tourist_attraction\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "\n",
        "    response = requests.get(url, params=params).json()\n",
        "    if \"results\" in response:\n",
        "        return [f\"{t['name']} ({t.get('rating', 'No rating')}⭐)\" for t in response[\"results\"][:top_n]]\n",
        "    return \"No tourist attractions found.\"\n",
        "\n",
        "# ✅ Function to fetch restaurants\n",
        "def fetch_restaurants(location, purpose, top_n=5):\n",
        "    lat, lng = get_lat_lng(location)\n",
        "    if not lat or not lng:\n",
        "        return \"Could not determine the exact location.\"\n",
        "\n",
        "    keyword = {\n",
        "        \"Leisure\": \"casual dining\",\n",
        "        \"Business\": \"fine dining\",\n",
        "        \"Family\": \"family-friendly\",\n",
        "        \"Adventure\": \"unique cuisine\",\n",
        "        \"Romantic\": \"romantic restaurant\"\n",
        "    }.get(purpose, \"restaurant\")\n",
        "\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\"location\": f\"{lat},{lng}\", \"radius\": 5000, \"type\": \"restaurant\", \"keyword\": keyword, \"key\": GOOGLE_MAPS_API_KEY}\n",
        "\n",
        "    response = requests.get(url, params=params).json()\n",
        "    if \"results\" in response:\n",
        "        return [f\"{r['name']} ({r.get('rating', 'No rating')}⭐)\" for r in response[\"results\"][:top_n]]\n",
        "    return \"No restaurants found.\"\n",
        "\n",
        "# ✅ Function to fetch real-time weather details\n",
        "def fetch_weather(city):\n",
        "    url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\"q\": city, \"appid\": WEATHER_API_KEY, \"units\": \"metric\"}\n",
        "    response = requests.get(url, params=params).json()\n",
        "    if \"weather\" in response and \"main\" in response:\n",
        "        return f\"{response['weather'][0]['description'].capitalize()}, {response['main']['temp']}°C\"\n",
        "    return \"Weather data not available.\"\n",
        "\n",
        "def get_airport_code(city_name):\n",
        "    \"\"\"\n",
        "    Convert a city name to an airport code using the Amadeus API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = amadeus.reference_data.locations.get(\n",
        "            keyword=city_name,\n",
        "            subType='AIRPORT'\n",
        "        )\n",
        "        # Extract the airport code from the response\n",
        "        for location in response.data:\n",
        "            if location['subType'] == 'AIRPORT':\n",
        "                return location['iataCode']\n",
        "        return None\n",
        "    except ResponseError as error:\n",
        "        print(f\"Error fetching airport code for {city_name}: {error}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Helper Function to get full airline names from its codes using OpenAI\n",
        "def get_airline_full_name(airline_code):\n",
        "    prompt = f\"Please provide only the full name for the airline '{airline_code}'.\"\n",
        "    response = llm([HumanMessage(content=prompt)])\n",
        "    return response.content.strip() if response else airline_code  # Return the code if response is empty\n",
        "\n",
        "def format_duration(iso_duration):\n",
        "    match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?', iso_duration)\n",
        "    hours = match.group(1) if match.group(1) else \"0\"\n",
        "    minutes = match.group(2) if match.group(2) else \"0\"\n",
        "    return f\"{int(hours)} hours {int(minutes)} minutes\"\n",
        "\n",
        "# Function to fetch Flights information\n",
        "def fetch_flight_details(origin, destination, start_data, return_date=None, max_price=None, airline_name =None):\n",
        "    try:\n",
        "        origin_code = get_airport_code(origin)\n",
        "        destination_code = get_airport_code(destination)\n",
        "        # Set a high default max_price if not provided\n",
        "        max_price = max_price if max_price else 20000\n",
        "        params = {\n",
        "            \"originLocationCode\": origin_code,\n",
        "            \"destinationLocationCode\": destination_code,\n",
        "            \"departureDate\": start_data,\n",
        "            \"adults\": 1,\n",
        "            \"maxPrice\": max_price\n",
        "        }\n",
        "\n",
        "        if return_date:\n",
        "            params[\"returnDate\"] = return_date  # Include return date for round-trip flights\n",
        "\n",
        "        # Fetch flights from Amadeus API\n",
        "        response = amadeus.shopping.flight_offers_search.get(**params)\n",
        "        flights = response.data\n",
        "\n",
        "        if flights:\n",
        "            result = []\n",
        "            for flight in flights[:5]:  # Limit to top 5 results\n",
        "                if float(flight['price']['total']) <= max_price:\n",
        "                    # Outbound flight details\n",
        "                    segments = flight['itineraries'][0]['segments']\n",
        "                    airline_code = segments[0]['carrierCode']\n",
        "                    airline = get_airline_full_name(airline_code)  # Get full airline name\n",
        "                    # Only add flights that match the specified airline, if provided\n",
        "                    if airline_name and airline and airline.lower() not in airline_name.lower():\n",
        "                        continue\n",
        "                    departure_time = segments[0]['departure']['at']\n",
        "                    arrival_time = segments[-1]['arrival']['at']\n",
        "                    flight_duration = format_duration(flight['itineraries'][0]['duration'])\n",
        "\n",
        "                    # Only include return details if a return date is provided\n",
        "                    if return_date and len(flight['itineraries']) > 1:\n",
        "                        return_segments = flight['itineraries'][1]['segments']\n",
        "                        return_departure_time = return_segments[0]['departure']['at']\n",
        "                        return_arrival_time = return_segments[-1]['arrival']['at']\n",
        "                        return_duration = format_duration(flight['itineraries'][1]['duration'])\n",
        "                        return_info = (\n",
        "                            f\"\\nReturn Departure: {return_departure_time}\\n\"\n",
        "                            f\"Return Arrival: {return_arrival_time}\\n\"\n",
        "                            f\"Return Duration: {return_duration}\\n\"\n",
        "                        )\n",
        "                    else:\n",
        "                        return_info = \"\"\n",
        "\n",
        "                    # Append both outbound and return information (if available) to results\n",
        "                    result.append(\n",
        "                        f\"Airline: {airline}\\nPrice: ${flight['price']['total']}\\n\"\n",
        "                        f\"Departure: {departure_time}\\nArrival: {arrival_time}\\n\"\n",
        "                        f\"Duration: {flight_duration}{return_info}\"\n",
        "                        \"\\n----------------------------------------\"\n",
        "                    )\n",
        "            return \"\\n\\n\".join(result) if result else \"No flights found within the budget.\"\n",
        "        return \"No flights found.\"\n",
        "    except ResponseError as error:\n",
        "        return f\"An error occurred: {error.response.result}\"\n",
        "\n",
        "# ✅ Function to fetch hotels\n",
        "def fetch_hotels(location, top_n=5):\n",
        "    lat, lng = get_lat_lng(location)\n",
        "    if not lat or not lng:\n",
        "        return \"Could not determine the exact location.\"\n",
        "\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\"location\": f\"{lat},{lng}\", \"radius\": 5000, \"type\": \"lodging\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "\n",
        "    response = requests.get(url, params=params).json()\n",
        "    if \"results\" in response:\n",
        "        return [f\"{h['name']} ({h.get('rating', 'No rating')}⭐)\" for h in response[\"results\"][:top_n]]\n",
        "    return \"No hotels found.\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOZmyouRX3O1",
        "outputId": "f9a46ab0-4995-4222-f215-519592b5fab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write travel_story.py (AI-Powered Travel Story & Video Generation)**"
      ],
      "metadata": {
        "id": "0LGfRny-Z5gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile travel_story.py\n",
        "\n",
        "import openai\n",
        "import requests\n",
        "import wikipediaapi\n",
        "import io\n",
        "import time\n",
        "import onnx\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# from moviepy.editor import *  # For video editing\n",
        "from gtts import gTTS  # For text-to-speech\n",
        "from onnxruntime import InferenceSession\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from config import OPENAI_API_KEY # Import the latest API key from config.py\n",
        "from config import llm, amadeus\n",
        "import openai\n",
        "import faiss\n",
        "from moviepy.editor import AudioFileClip, ImageClip, concatenate_videoclips\n",
        "from utils import update_faiss_index , generate_travel_story_rag, generate_travel_plan_rag, retrieve_relevant_docs\n",
        "from io import BytesIO\n",
        "\n",
        "# ✅ Ensure OpenAI API is using the latest key\n",
        "openai.api_key = OPENAI_API_KEY  # This ensures the latest key is used every time\n",
        "\n",
        "# ✅ Initialize OpenAI Client\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# ✅ Wikipedia API Setup\n",
        "wiki = wikipediaapi.Wikipedia(user_agent=\"MyTravelApp/1.0\", language=\"en\")\n",
        "\n",
        "# ✅ Function to fetch travel data from Wikipedia\n",
        "def get_wikipedia_summary(place):\n",
        "    page = wiki.page(place)\n",
        "    return page.summary[:500] if page.exists() else \"No Wikipedia summary found.\"\n",
        "\n",
        "def generate_travel_story(origin, destination, purpose, start_date, end_date):\n",
        "    # wikipedia_info = get_wikipedia_summary(destination)\n",
        "    update_faiss_index(destination)\n",
        "    wikipedia_info = generate_travel_story_rag(origin, destination, start_date, end_date, purpose)\n",
        "\n",
        "    purpose_templates = {\n",
        "      \"leisure\": f\"You are about to embark on a relaxing leisure trip starting from {origin} to {destination} from {start_date} to {end_date}. Describe your journey, including your departure experience, flight details, and how you arrive at {destination}. Highlight the famous landmarks, scenic parks, and peaceful experiences during your visit.\",\n",
        "\n",
        "      \"food\": f\"As a food lover, you're traveling from {origin} to {destination} from {start_date} to {end_date} to explore its vibrant culinary scene. Describe the unique food experiences from the departure airport to your destination, including delicious street food, high-end restaurants, and bustling food markets in {destination}. Mention iconic cafés and dishes travelers should not miss.\",\n",
        "\n",
        "      \"adventure\": f\"You're departing from {origin} to {destination} for an adrenaline-filled adventure from {start_date} to {end_date}. Describe your travel experience, flight, and arrival at {destination}. Highlight the thrilling activities such as hiking, surfing, skydiving, and other outdoor experiences that make this trip exhilarating.\",\n",
        "\n",
        "      \"business\": f\"You're traveling from {origin} to {destination} for a business trip from {start_date} to {end_date}. Describe your departure from {origin}, your flight experience, and how you arrive at {destination}. Detail your meetings, networking events, and the city's corporate atmosphere. Also, mention after-hours dining or sightseeing to balance work and leisure.\",\n",
        "\n",
        "      \"romantic\": f\"You're setting off from {origin} to {destination} for a romantic getaway from {start_date} to {end_date}. Describe the journey from {origin}, including your travel experience and how you and your partner arrive at {destination}. Highlight intimate dinners, scenic walks, breathtaking sunset views, and special moments shared during this trip.\",\n",
        "\n",
        "      \"spiritual\": f\"You're traveling from {origin} to {destination} for a spiritual retreat from {start_date} to {end_date}. Describe your departure experience from {origin}, flight details, and arrival at {destination}. Mention meditation spots, temples, churches, and peaceful landscapes that provide a sense of tranquility and reflection.\",\n",
        "\n",
        "      \"family\": f\"You're taking a family trip from {origin} to {destination} from {start_date} to {end_date}, creating memorable bonding moments. Describe your journey, including how you and your family prepare for the trip, your flight experience, and your arrival at {destination}. Highlight amusement parks, kid-friendly attractions, and activities that make this a joyful and unforgettable experience for everyone.\"\n",
        "    }\n",
        "\n",
        "\n",
        "    purpose_prompt = purpose_templates.get(purpose, purpose_templates[\"leisure\"])\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {purpose_prompt}\n",
        "\n",
        "    Be immersive, engaging, and detailed. Use vivid descriptions and include unique aspects of {destination}.\n",
        "\n",
        "    Wikipedia Summary: {wikipedia_info}\n",
        "\n",
        "    Travel Story:\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(full_prompt)\n",
        "    return response.content if hasattr(response, 'content') else str(response)\n",
        "\n",
        "# ✅ Function to generate a travel plan\n",
        "def generate_travel_plan(origin, destination, start_date, end_date, purpose):\n",
        "    update_faiss_index(destination)\n",
        "    travel_plan_context = generate_travel_plan_rag(origin, destination, start_date, end_date, purpose)\n",
        "    prompt = f\"\"\"\n",
        "    Generate a detailed travel itinerary for a trip starting from {origin} to {destination} from {start_date} to {end_date} for {purpose}. Use {travel_plan_context} as a reference.\n",
        "\n",
        "    Include the following details:\n",
        "    - **Departure details** from {origin}, including flight or transportation options.\n",
        "    - **Arrival experience** in {destination} and first impressions.\n",
        "    - **Accommodation recommendations** suitable for {purpose}.\n",
        "    - **Top attractions** in {destination} that match {purpose}.\n",
        "    - **Food and dining recommendations**, including famous restaurants.\n",
        "    - **Local transportation options** to navigate within {destination}.\n",
        "    - **Return trip details** from {destination} back to {origin} (if applicable).\n",
        "\n",
        "    Ensure the itinerary is engaging and structured as a day-by-day plan.\n",
        "\n",
        "    Travel Itinerary:\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content if hasattr(response, 'content') else str(response)\n",
        "\n",
        "# ✅ Function to extract day-wise highlights\n",
        "def extract_daywise_highlights(travel_plan):\n",
        "    days = travel_plan.split(\"Day \")[1:]\n",
        "    daywise_highlights = {}\n",
        "    for day in days:\n",
        "        lines = day.split(\"\\n\")\n",
        "        day_number = lines[0].strip()\n",
        "        activities = \". \".join([line.strip() for line in lines[1:] if line.strip()])\n",
        "        daywise_highlights[day_number] = activities\n",
        "    return daywise_highlights\n",
        "\n",
        "# ✅ Function to generate travel images per day's activity\n",
        "def generate_travel_images(daywise_highlights, destination):\n",
        "    image_urls = {}\n",
        "    for day, activities in daywise_highlights.items():\n",
        "        prompt = f\"Generate an ultra-HD image representing {activities} in {destination}.\"\n",
        "        response = client.images.generate(\n",
        "            model=\"dall-e-3\", prompt=prompt, n=1, size=\"1024x1024\"\n",
        "        )\n",
        "        image_urls[day] = response.data[0].url\n",
        "    return image_urls\n",
        "\n",
        "# ✅ Function to generate a voice-over for the itinerary\n",
        "def generate_voiceover(travel_plan, output_audio=\"travel_narration.mp3\"):\n",
        "    tts = gTTS(text=travel_plan, lang=\"en\", slow=False)\n",
        "    tts.save(output_audio)\n",
        "    return output_audio\n",
        "\n",
        "def create_travel_video(image_urls, narration_audio, output_video=\"travel_story.mp4\"):\n",
        "    # Load the narration audio\n",
        "    audio_clip = AudioFileClip(narration_audio)\n",
        "    total_audio_duration = audio_clip.duration\n",
        "    num_days = len(image_urls)\n",
        "    image_duration = total_audio_duration / num_days  # Divide equally among days\n",
        "\n",
        "    image_clips = []\n",
        "    for day, url in image_urls.items():\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "            image_path = f\"travel_image_{day}.jpg\"\n",
        "            image.save(image_path)\n",
        "\n",
        "            # Create an ImageClip with the specified duration\n",
        "            clip = ImageClip(image_path, duration=image_duration).set_fps(24)\n",
        "            clip = clip.resize(lambda t: 1 + 0.01 * t)  # Slow zoom-in effect\n",
        "            image_clips.append(clip)\n",
        "        else:\n",
        "            print(f\"Unable to fetch image for Day {day}. Status code: {response.status_code}\")\n",
        "\n",
        "    # Concatenate all image clips into a single video\n",
        "    video_clip = concatenate_videoclips(image_clips, method=\"compose\")\n",
        "    video_clip = video_clip.set_audio(audio_clip)\n",
        "\n",
        "    # Write the video file\n",
        "    video_clip.write_videofile(output_video, codec=\"libx264\", fps=24, audio_codec=\"aac\")\n",
        "    print(\"🎬 Personalized travel video created successfully!\")\n",
        "    return output_video\n",
        "\n",
        "# ✅ Execution for Testing\n",
        "if __name__ == \"__main__\":\n",
        "    origin = \"New York\"\n",
        "    destination = \"Hyderabad\"\n",
        "    purpose = \"Family\"\n",
        "    start_date = \"2025-04-15\"\n",
        "    end_date = \"2025-04-20\"\n",
        "\n",
        "    print(\"📅 Generating Travel Plan...\")\n",
        "    travel_plan = generate_travel_plan(origin, destination, start_date, end_date, purpose)\n",
        "    print(f\"📝 Travel Plan:\\n{travel_plan}\")\n",
        "\n",
        "    # ✅ Extract day-wise activities\n",
        "    daywise_highlights = extract_daywise_highlights(travel_plan)\n",
        "\n",
        "    print(\"🔄 Fetching data and generating travel story...\")\n",
        "    travel_story_text = generate_travel_story(origin, destination, purpose, start_date, end_date)\n",
        "    print(f\"📖 Travel Story:\\n{travel_story_text}\")\n",
        "\n",
        "    print(\"🖼 Generating Travel Images...\")\n",
        "    image_urls = generate_travel_images(daywise_highlights, destination)\n",
        "    print(f\"Generated Images: {image_urls}\")\n",
        "\n",
        "    print(\"🎤 Generating voiceover...\")\n",
        "    narration_file = generate_voiceover(travel_story_text)\n",
        "\n",
        "    print(\"🎥 Creating travel video...\")\n",
        "    travel_video = create_travel_video(image_urls, narration_file)\n",
        "    print(f\"✅ Travel video saved as {travel_video}\")\n"
      ],
      "metadata": {
        "id": "rtPSz65JZXj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835aa668-9219-4386-d873-0dec59059fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting travel_story.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from travel_story import generate_travel_story, generate_travel_plan\n",
        "\n",
        "# travel_story_text = generate_travel_story(\"Hyderabad\", \"Paris\", \"Family\", \"2025-04-25\", \"2025-04-30\")\n",
        "travel_plan_text = generate_travel_plan(\"Hyderabad\", \"Paris\", \"2025-04-25\", \"2025-04-30\", \"Family\")\n",
        "print(travel_plan_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxWTNlXxISJ3",
        "outputId": "3cdf16bb-5eb8-45d0-9c10-e2d6d3384dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Updating FAISS index for Paris...\n",
            "Fetching travel data for Paris...\n",
            "✅ FAISS Index successfully updated with travel data for Paris\n",
            "🔄 Generating AI Travel Plan using RAG...\n",
            "🔍 Retrieving relevant documents for query: Best travel itinerary for Paris\n",
            "✅ Relevant Documents Found: [(Document(metadata={'doc_id': '0'}, page_content=\"Paris is the capital and largest city of France. With an estimated population of 2,048,472 residents in January 2025 in an area of more than 105\\xa0km2 (41\\xa0sq\\xa0mi), Paris is the fourth-most populous city in the European Union and the 30th most densely populated city in the world in 2022. Since the 17th century, Paris has been one of the world's major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. Because of its leading role in the arts and sciences and its early adaptation of extensive street lighting, Paris became known as the City of Light in the 19th century.\\n\\nTop Attractions:\\nGrand Palais - 75008 Paris, France\\nArc de Triomphe - Pl. Charles de Gaulle, 75008 Paris, France\\nNotre-Dame Cathedral of Paris - 6 Parvis Notre-Dame - Pl. Jean-Paul II, 75004 Paris, France\\nEiffel Tower - Av. Gustave Eiffel, 75007 Paris, France\\nOverlook of Paris - 5 Rue du Cardinal Dubois, 75018 Paris, France\"), np.float32(0.3289693)), (Document(metadata={'doc_id': '1'}, page_content=\"Paris is the capital and largest city of France. With an estimated population of 2,048,472 residents in January 2025 in an area of more than 105\\xa0km2 (41\\xa0sq\\xa0mi), Paris is the fourth-most populous city in the European Union and the 30th most densely populated city in the world in 2022. Since the 17th century, Paris has been one of the world's major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. Because of its leading role in the arts and sciences and its early adaptation of extensive street lighting, Paris became known as the City of Light in the 19th century.\\n\\nTop Attractions:\\nGrand Palais - 75008 Paris, France\\nArc de Triomphe - Pl. Charles de Gaulle, 75008 Paris, France\\nNotre-Dame Cathedral of Paris - 6 Parvis Notre-Dame - Pl. Jean-Paul II, 75004 Paris, France\\nEiffel Tower - Av. Gustave Eiffel, 75007 Paris, France\\nOverlook of Paris - 5 Rue du Cardinal Dubois, 75018 Paris, France\"), np.float32(0.3289693))]\n",
            "Day 1: 2025-04-25\n",
            "- Depart from Rajiv Gandhi International Airport, Hyderabad at 10:00 AM via Air France Flight AF 226. Enjoy the in-flight services and amenities.\n",
            "- Arrive at Charles De Gaulle Airport, Paris at 6:30 PM local time. Experience the bustling atmosphere of one of the world's busiest airports.\n",
            "- Transfer to the hotel (Hotel Lutetia, 45 Boulevard Raspail, 75006 Paris, France) via a pre-booked taxi or public transportation. This luxury hotel is family-friendly and located in the heart of Paris.\n",
            "- Have dinner at the hotel's restaurant, which offers a variety of French and international cuisines. Rest for the day to recover from jet lag.\n",
            "\n",
            "Day 2: 2025-04-26\n",
            "- Enjoy a delicious breakfast at the hotel.\n",
            "- Visit the Grand Palais (75008 Paris, France) at 10:00 AM. This historic site hosts various exhibitions and events throughout the year.\n",
            "- Have lunch at a local restaurant. Try some traditional French dishes like croissants, baguettes, and escargot.\n",
            "- Visit the Arc de Triomphe (Pl. Charles de Gaulle, 75008 Paris, France) at 2:00 PM. Climb to the top for a panoramic view of Paris.\n",
            "- Return to the hotel and freshen up.\n",
            "- Have dinner at a local restaurant. Try some French wine and cheese.\n",
            "\n",
            "Day 3: 2025-04-27\n",
            "- Breakfast at the hotel.\n",
            "- Visit the Notre-Dame Cathedral of Paris (6 Parvis Notre-Dame - Pl. Jean-Paul II, 75004 Paris, France) at 10:00 AM. Explore the iconic cathedral and its surroundings.\n",
            "- Lunch at a local restaurant. Try some French pastries for dessert.\n",
            "- Visit the Eiffel Tower (Av. Gustave Eiffel, 75007 Paris, France) at 2:00 PM. Take the elevator to the top and enjoy the view of Paris.\n",
            "- Return to the hotel and freshen up.\n",
            "- Dinner at a local restaurant. Try some French seafood dishes.\n",
            "\n",
            "Day 4: 2025-04-28\n",
            "- Breakfast at the hotel.\n",
            "- Visit the Overlook of Paris (5 Rue du Cardinal Dubois, 75018 Paris, France) at 10:00 AM. This is a great spot for family photos.\n",
            "- Lunch at a local restaurant. Try some French crepes.\n",
            "- Spend the afternoon exploring local shops and markets. Buy some souvenirs for friends and family back home.\n",
            "- Return to the hotel and freshen up.\n",
            "- Dinner at a local restaurant. Try some French steak and fries.\n",
            "\n",
            "Day 5: 2025-04-29\n",
            "- Breakfast at the hotel.\n",
            "- Free day for personal activities, shopping, or exploring other attractions. You can visit the Louvre Museum, Disneyland Paris, or the Palace of Versailles.\n",
            "- Lunch and dinner at local restaurants. Try some French desserts like macarons and crème brûlée.\n",
            "\n",
            "Day 6: 2025-04-30\n",
            "- Breakfast at the hotel.\n",
            "- Check-out from the hotel and head to Charles De Gaulle Airport via a pre-booked taxi or public transportation.\n",
            "- Depart from Charles De Gaulle Airport, Paris at 12:00 PM via Air France Flight AF 225.\n",
            "- Arrive at Rajiv Gandhi International Airport, Hyderabad at 1:30 AM (next day).\n",
            "\n",
            "Note: This itinerary is subject to change based on flight schedules, weather conditions, and personal preferences. Use this itinerary as a reference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write app.py (Streamlit UI)**"
      ],
      "metadata": {
        "id": "O_6GvknqdZt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from travel_story import generate_travel_story, generate_voiceover, create_travel_video, generate_travel_plan, generate_travel_images, extract_daywise_highlights\n",
        "from utils import fetch_weather, fetch_tourist_attractions, fetch_flight_details, fetch_restaurants, fetch_hotels\n",
        "from config import llm  # Ensure llm is imported for LLM calls\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Set up Streamlit Page\n",
        "st.set_page_config(page_title=\"✈️ AI Travel Planner\", layout=\"wide\")\n",
        "\n",
        "st.title(\"✈️ AI Travel Planner 🏨\")\n",
        "\n",
        "# Sidebar Inputs\n",
        "st.sidebar.title(\"Plan Your Trip 🗺\")\n",
        "origin = st.sidebar.text_input(\"Enter Origin City\", \"New York\")\n",
        "destination = st.sidebar.text_input(\"Enter Destination\", \"Paris\")\n",
        "start_date = st.sidebar.date_input(\"Start Date\")\n",
        "end_date = st.sidebar.date_input(\"End Date\")\n",
        "purpose = st.sidebar.selectbox(\"Purpose of Visit\", [\"Leisure\", \"Business\", \"Adventure\", \"Romantic\", \"Family\"])\n",
        "\n",
        "# Initialize session state for storing travel plan and conversation\n",
        "if \"travel_plan\" not in st.session_state:\n",
        "    st.session_state.travel_plan = None\n",
        "    st.session_state.travel_details = {}\n",
        "\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = []\n",
        "\n",
        "# Keep track of message processing state\n",
        "if \"message_to_process\" not in st.session_state:\n",
        "    st.session_state.message_to_process = None\n",
        "\n",
        "# Generate Travel Plan & Fetch Details\n",
        "if st.sidebar.button(\"Generate Travel Plan & Details\", key=\"gen_plan_btn\"):\n",
        "    with st.spinner(\"🔄 Generating AI travel plan & fetching details...\"):\n",
        "        travel_plan_text = generate_travel_plan(origin, destination, start_date, end_date, purpose)\n",
        "        weather_info = fetch_weather(destination)\n",
        "        tourist_attractions = fetch_tourist_attractions(destination)\n",
        "        restaurants = fetch_restaurants(destination, purpose)\n",
        "        hotels = fetch_hotels(destination)\n",
        "        flights = fetch_flight_details(origin, destination, start_date, return_date=None, max_price=None, airline_name=None)\n",
        "\n",
        "        # Store the travel plan in session state for the chatbot to use\n",
        "        st.session_state.travel_plan = travel_plan_text\n",
        "        st.session_state.travel_details = {\n",
        "            \"weather\": weather_info,\n",
        "            \"attractions\": tourist_attractions,\n",
        "            \"restaurants\": restaurants,\n",
        "            \"hotels\": hotels,\n",
        "            \"flights\": flights\n",
        "        }\n",
        "\n",
        "    # Display the generated plan and details\n",
        "    st.subheader(\"📅 Your AI-Generated Travel Plan\")\n",
        "    st.write(travel_plan_text)\n",
        "    st.subheader(f\"🌦 Weather Forecast in {destination}\")\n",
        "    st.write(weather_info)\n",
        "    st.subheader(f\"🏛 Top Attractions in {destination}\")\n",
        "    st.write(tourist_attractions)\n",
        "    st.subheader(f\"🍽 Best Restaurants in {destination}\")\n",
        "    st.write(restaurants)\n",
        "    st.subheader(f\"🏨 Recommended Hotels in {destination}\")\n",
        "    st.write(hotels)\n",
        "    st.subheader(f\"✈️ Flights from {origin} to {destination}\")\n",
        "    st.write(flights)\n",
        "\n",
        "# Generate Travel Story & Voiceover\n",
        "if st.sidebar.button(\"Generate Story & Voiceover\", key=\"gen_story_btn\"):\n",
        "    with st.spinner(\"🔄 Generating AI travel story...\"):\n",
        "        travel_story_text = generate_travel_story(origin, destination, purpose, start_date, end_date)\n",
        "        narration_audio = generate_voiceover(travel_story_text)\n",
        "\n",
        "    st.subheader(\"📖 Your AI-Generated Travel Story\")\n",
        "    st.write(travel_story_text)\n",
        "    st.subheader(\"🎤 AI Voiceover\")\n",
        "    st.audio(narration_audio)\n",
        "\n",
        "# Generate Images & Video\n",
        "if st.sidebar.button(\"Generate Images & Video\", key=\"gen_media_btn\"):\n",
        "    with st.spinner(\"🖼 Generating Travel Images...\"):\n",
        "        travel_plan_text = generate_travel_plan(origin, destination, start_date, end_date, purpose)\n",
        "        daywise_highlights = extract_daywise_highlights(travel_plan_text)\n",
        "        travel_images = generate_travel_images(daywise_highlights, destination)\n",
        "\n",
        "    st.subheader(\"🖼 View Destination Images\")\n",
        "    if travel_images:\n",
        "        for day, image_url in travel_images.items():\n",
        "            try:\n",
        "                response = requests.get(image_url)\n",
        "                if response.status_code == 200:\n",
        "                    image = Image.open(BytesIO(response.content))\n",
        "                    st.image(image, caption=f\"Day {day}: A view of {destination}\", use_column_width=True)\n",
        "                else:\n",
        "                    st.warning(f\"❌ Unable to fetch image for Day {day}. Try again later.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching image for Day {day}: {e}\")\n",
        "    else:\n",
        "        st.warning(\"❌ No images generated.\")\n",
        "\n",
        "    with st.spinner(\"🎥 Creating AI Travel Video...\"):\n",
        "        travel_plan_text = generate_travel_plan(origin, destination, start_date, end_date, purpose)\n",
        "        daywise_highlights = extract_daywise_highlights(travel_plan_text)\n",
        "        travel_images = generate_travel_images(daywise_highlights, destination)\n",
        "        narration_audio = generate_voiceover(travel_plan_text)\n",
        "        travel_video = create_travel_video(travel_images, narration_audio)\n",
        "\n",
        "    st.subheader(\"🎥 AI-Generated Travel Video\")\n",
        "    if travel_video:\n",
        "        st.video(travel_video)\n",
        "    else:\n",
        "        st.warning(\"❌ Video generation failed.\")\n",
        "\n",
        "st.success(\"✅ AI Travel Planner is ready!\")\n",
        "\n",
        "# =======================\n",
        "# Interactive Chat Section\n",
        "# =======================\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.header(\"🤖 Chat with Your Travel Planner\")\n",
        "st.write(\"Ask questions about your trip or request modifications to your plan.\")\n",
        "\n",
        "\n",
        "def update_message_to_process():\n",
        "# First, check if there's a message to process from the previous run\n",
        "    if st.session_state.message_to_process:\n",
        "        with st.spinner(\"Processing your message...\"):\n",
        "            # Get the message to process\n",
        "            message = st.session_state.message_to_process\n",
        "\n",
        "            # Add user message to conversation\n",
        "            st.session_state.conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "            # Build context for the LLM based on existing travel information\n",
        "            context = f\"Origin: {origin}, Destination: {destination}, Dates: {start_date} to {end_date}, Purpose: {purpose}\"\n",
        "\n",
        "            # Create a comprehensive system prompt that includes the base travel plan\n",
        "            travel_plan = st.session_state.travel_plan if st.session_state.travel_plan else \"No travel plan has been generated yet.\"\n",
        "            travel_details = st.session_state.travel_details\n",
        "\n",
        "            system_prompt = f\"\"\"You are an AI travel assistant helping a user with their trip.\n",
        "\n",
        "            TRIP DETAILS:\n",
        "            - Origin: {origin}\n",
        "            - Destination: {destination}\n",
        "            - Dates: {start_date} to {end_date}\n",
        "            - Purpose: {purpose}\n",
        "\n",
        "            BASE TRAVEL PLAN:\n",
        "            {travel_plan}\n",
        "\n",
        "            ADDITIONAL INFORMATION:\n",
        "            - Weather: {travel_details.get('weather', 'Not available')}\n",
        "            - Top Attractions: {travel_details.get('attractions', 'Not available')}\n",
        "            - Restaurants: {travel_details.get('restaurants', 'Not available')}\n",
        "            - Hotels: {travel_details.get('hotels', 'Not available')}\n",
        "            - Flights: {travel_details.get('flights', 'Not available')}\n",
        "\n",
        "            INSTRUCTIONS:\n",
        "            - Remember all details about the user's trip when answering questions\n",
        "            - Be concise but informative in your responses\n",
        "            - If the user asks about information not in the plan, respond with relevant suggestions\n",
        "            - If the user wants to modify their plan, acknowledge this and explain how the modification fits with the overall trip\n",
        "            \"\"\"\n",
        "\n",
        "            # Create messages array for the LLM\n",
        "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "            # Add conversation history to messages\n",
        "            for msg in st.session_state.conversation:\n",
        "                if msg[\"role\"] != \"system\":  # Avoid duplicate system messages\n",
        "                    messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
        "\n",
        "            # Generate assistant response using the LLM\n",
        "            try:\n",
        "                # Using the llm in a way that works with both older and newer LLM interfaces\n",
        "                if hasattr(llm, 'chat'):\n",
        "                    # For newer LLM interfaces that use the chat method\n",
        "                    response = llm.chat(messages)\n",
        "                    assistant_response = response.content if hasattr(response, \"content\") else str(response)\n",
        "                else:\n",
        "                    # For older LLM interfaces or those using direct invoke\n",
        "                    full_prompt = system_prompt + \"\\n\\n\" + \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages if m['role'] != \"system\"])\n",
        "                    response = llm.invoke(full_prompt)\n",
        "                    assistant_response = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "                # Add assistant response to conversation history\n",
        "                st.session_state.conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating response: {str(e)}\")\n",
        "\n",
        "            # Clear the message to process\n",
        "            st.session_state.message_to_process = None\n",
        "\n",
        "# Display the conversation history\n",
        "st.subheader(\"💬 Your Conversation\")\n",
        "for message in st.session_state.conversation:\n",
        "    if message[\"role\"] == \"user\":\n",
        "        st.markdown(f\"**You:** {message['content']}\")\n",
        "    else:\n",
        "        st.markdown(f\"**Planner:** {message['content']}\")\n",
        "\n",
        "# # Input for user's message\n",
        "# user_input = st.text_input(\"Your message:\", key=\"chat_input\")\n",
        "# send_button = st.button(\"Send\", key=\"send_btn\")\n",
        "\n",
        "# if send_button and user_input:\n",
        "#     # Store the message to process in the next run\n",
        "#     st.session_state.message_to_process = user_input\n",
        "#     # # Clear the input\n",
        "#     # st.session_state.chat_input = \"\"\n",
        "#     update_message_to_process()\n",
        "\n",
        "# Initialize the key in session state if it doesn't exist\n",
        "if \"user_input\" not in st.session_state:\n",
        "    st.session_state.user_input = \"\"\n",
        "\n",
        "# Input for user's message - use a callback to handle submissions\n",
        "def submit_message():\n",
        "    if st.session_state.user_input.strip():\n",
        "        st.session_state.message_to_process = st.session_state.user_input\n",
        "        st.session_state.user_input = \"\"\n",
        "        update_message_to_process()\n",
        "\n",
        "# Create the text input with the callback\n",
        "user_input = st.text_input(\n",
        "    \"Your message:\",\n",
        "    key=\"user_input\",\n",
        "    on_change=submit_message\n",
        ")\n",
        "\n",
        "# # Add a send button that also triggers the same callback\n",
        "# if st.button(\"Send\", key=\"send_btn\"):\n",
        "#     submit_message()\n"
      ],
      "metadata": {
        "id": "qFv-DW57bNRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe286f0-7746-4e91-9336-3443a9a529a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from travel_story import generate_travel_story, generate_voiceover, create_travel_video, generate_travel_plan, generate_travel_images\n",
        "# from utils import fetch_weather, fetch_tourist_attractions, fetch_flight_details, fetch_restaurants, fetch_hotels\n",
        "# from config import llm\n",
        "\n",
        "\n",
        "# class SessionState:\n",
        "#     def __init__(self):\n",
        "#         self.conversation = []\n",
        "\n",
        "# # Initialize session state\n",
        "# session_state = SessionState()\n",
        "\n",
        "# travel_plan_text = generate_travel_plan(\"New York\", \"New Delhi\", \"2025-04-15\", \"2025-04-20\", \"Family\")\n",
        "# print(travel_plan_text)\n",
        "\n",
        "# def chatbot_loop():\n",
        "#     \"\"\"\n",
        "#     Simulates a chatbot interaction where users provide input,\n",
        "#     and the chatbot refines the travel plan iteratively.\n",
        "#     \"\"\"\n",
        "#     print(\"🤖 AI Travel Planner Chatbot\")\n",
        "#     print(\"Type 'exit' to stop the chat.\\n\")\n",
        "\n",
        "#     while True:\n",
        "#         user_input = input(\"You: \")\n",
        "#         if user_input.lower() == \"exit\":\n",
        "#             print(\"\\nChat ended.\")\n",
        "#             break\n",
        "\n",
        "#         # Append user message to conversation history\n",
        "#         session_state.conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "#         # Construct conversation context\n",
        "#         conversation_context = \"\\n\".join(\n",
        "#             [f\"{msg['role']}: {msg['content']}\" for msg in session_state.conversation]\n",
        "#         )\n",
        "#         # print(conversation_context)\n",
        "\n",
        "#         prompt = (\n",
        "#             f\"Below is the base travel plan:\\n{travel_plan_text}\\n\\n\"\n",
        "#             f\"And here is the conversation with the user:\\n{conversation_context}\\n\\n\"\n",
        "#             \"Based on the above, please respond to the user answering his queries.\"\n",
        "#         )\n",
        "\n",
        "#         # Mock LLM call (replace with actual LLM API call in production)\n",
        "#         ai_response = llm.invoke(prompt)\n",
        "\n",
        "#         # Append AI response to conversation history\n",
        "#         session_state.conversation.append({\"role\": \"assistant\", \"content\": ai_response})\n",
        "\n",
        "#         # Print the response\n",
        "#         print(f\"Planner: {ai_response}\\n\")\n",
        "\n",
        "\n",
        "# # Run the chatbot loop\n",
        "# chatbot_loop()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3MU_fp01VvBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verify_imports.py\n",
        "\n",
        "import os\n",
        "\n",
        "# ✅ Check if all necessary files exist\n",
        "required_files = [\"config.py\", \"utils.py\", \"travel_story.py\", \"app.py\"]\n",
        "missing_files = [file for file in required_files if not os.path.exists(file)]\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"❌ ERROR: Missing files: {missing_files}. Ensure all required files are present.\")\n",
        "else:\n",
        "    print(\"✅ All necessary files exist.\")\n",
        "\n",
        "# ✅ Verify `config.py` imports\n",
        "try:\n",
        "    from config import google_maps_api_key, serpapi_key, WEATHER_API_KEY\n",
        "    print(\"✅ Successfully imported API keys from config.py\")\n",
        "    print(f\"Google Maps API Key: {google_maps_api_key[:5]}******\")\n",
        "    print(f\"SerpAPI Key: {serpapi_key[:5]}******\")\n",
        "    print(f\"Weather API Key: {weather_api_key[:5]}******\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"❌ ERROR: 'config.py' not found.\")\n",
        "except ImportError:\n",
        "    print(\"❌ ERROR: Could not import variables from 'config.py'.\")\n",
        "\n",
        "# ✅ Verify `utils.py` imports\n",
        "try:\n",
        "    from utils import get_lat_lng, fetch_restaurants, fetch_weather\n",
        "    print(\"✅ Successfully imported functions from utils.py\")\n",
        "    print(f\"get_lat_lng function exists: {callable(get_lat_lng)}\")\n",
        "    print(f\"fetch_restaurants function exists: {callable(fetch_restaurants)}\")\n",
        "    print(f\"fetch_weather function exists: {callable(fetch_weather)}\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"❌ ERROR: 'utils.py' not found.\")\n",
        "except ImportError:\n",
        "    print(\"❌ ERROR: Could not import functions from 'utils.py'.\")\n",
        "\n",
        "# ✅ Verify `purpose.py` imports\n",
        "try:\n",
        "    from travel_story import generate_travel_story, generate_voiceover, create_travel_video\n",
        "    print(\"✅ Successfully imported functions from purpose.py\")\n",
        "    print(f\"generate_travel_story function exists: {callable(generate_travel_story)}\")\n",
        "    print(f\"generate_voiceover function exists: {callable(generate_voiceover)}\")\n",
        "    print(f\"create_travel_video function exists: {callable(create_travel_video)}\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"❌ ERROR: 'purpose.py' not found.\")\n",
        "except ImportError:\n",
        "    print(\"❌ ERROR: Could not import functions from 'purpose.py'.\")\n",
        "\n",
        "# ✅ Verify `app.py` existence\n",
        "if os.path.exists(\"app.py\"):\n",
        "    print(\"✅ 'app.py' exists and is ready to run.\")\n",
        "else:\n",
        "    print(\"❌ ERROR: 'app.py' is missing.\")\n"
      ],
      "metadata": {
        "id": "8hKk65nvjqJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8e2df8-2f64-46fc-95a4-f4bbf6c267ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verify_imports.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "!kill $(pgrep -f ngrok)\n",
        "\n",
        "# Run Streamlit app\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "# Set up ngrok\n",
        "!ngrok authtoken 2sbDJyD6PkmqFpPwYo0r7FDObwZ_7cGS5ZDcbPxhQRJ6Swtzg\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &"
      ],
      "metadata": {
        "id": "TNg7J9YqcYLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc6065b-a9ae-4436-fdec-4846c10eb501"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "^C\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Public URL: NgrokTunnel: \"https://a7d7-34-91-103-28.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.103.28:8502\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from travel_story import generate_travel_story, generate_voiceover\n",
        "\n",
        "travel_story_text = generate_travel_story(\"New York\", \"New Delhi\", \"Family\", \"2025-04-15\", \"2025-04-20\")\n",
        "narration_audio = generate_voiceover(travel_story_text)\n",
        "\n",
        "y, sr = librosa.load(narration_audio, sr=None)\n",
        "\n",
        "# Plot the waveform\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveshow(y, sr=sr, alpha=0.6)\n",
        "plt.title(\"Voiceover Waveform\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHT2y3sZdbjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say you want to zoom in on the first 10 seconds\n",
        "zoom_start = 0\n",
        "zoom_end = int(10 * sr)  # 10 seconds worth of samples\n",
        "y_zoom = y[zoom_start:zoom_end]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveshow(y_zoom, sr=sr, alpha=0.6)\n",
        "plt.title(\"Zoomed-in Waveform (First 10 Seconds)\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qGOSdF_TN2mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute the Short-Time Fourier Transform (STFT)\n",
        "D = np.abs(librosa.stft(y))\n",
        "# Convert amplitude to decibels\n",
        "DB = librosa.amplitude_to_db(D, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(DB, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Spectrogram\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Frequency (Hz)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FZEYRgmrN9Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute RMS energy\n",
        "rms = librosa.feature.rms(y=y)[0]\n",
        "# Convert frame indices to time\n",
        "frames = range(len(rms))\n",
        "t = librosa.frames_to_time(frames, sr=sr)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot the raw waveform with reduced opacity\n",
        "librosa.display.waveshow(y, sr=sr, alpha=0.4, label='Waveform')\n",
        "\n",
        "# Overlay the RMS energy\n",
        "plt.plot(t, rms, color='r', label='RMS Energy')\n",
        "\n",
        "plt.title(\"Waveform with RMS Energy\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Amplitude / RMS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XYVXe9DNOK24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}